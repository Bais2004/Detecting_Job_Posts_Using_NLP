{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeF9w2N42XIDsf4RZm0fxD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bais2004/Detecting_Job_Posts_Using_NLP/blob/main/Detecting_Fake_job_post_using_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14-10-2025"
      ],
      "metadata": {
        "id": "yqTKM7TQG4o2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdHpIVQA374E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75c096af-e70f-4fad-815a-df9a8e46833c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Data', 'Science', 'is', 'fun', '!', 'My', 'name', 'is', 'Mritunjay', '.', 'I', 'am', 'from', 'Bhopal', '.', 'I', 'am', 'prusing', 'my', 'bachelors', 'degree', 'from', 'Technocrats', 'Institute', 'of', 'Technology', 'in', 'Artificial', 'Intelligence', 'And', 'Machine', 'Learning', '.']\n",
            "33\n"
          ]
        }
      ],
      "source": [
        "## TOKENIZATION\n",
        "import nltk # natural lang. toolkit\n",
        "# --- Corrected Code ---\n",
        "#nltk.download('punkt_tab')\n",
        "# Only 'punkt' is needed, 'punkt_tab' does not exist.\n",
        "nltk.download('punkt') # Download tokenizer data\n",
        "nltk.download('punkt_tab')\n",
        "# ----------------------\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample text\n",
        "text = \"Data Science is fun!\"\n",
        "text1= \"My name is Mritunjay. I am from Bhopal. I am prusing my bachelors degree from Technocrats Institute of Technology in Artificial Intelligence And Machine Learning.\"\n",
        "\n",
        "# Tokenize text\n",
        "tokens = word_tokenize(text)\n",
        "tokens1 = word_tokenize(text1)\n",
        "\n",
        "print(\"Tokens:\", tokens + tokens1)\n",
        "print(len(tokens + tokens1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15-10-2025\n"
      ],
      "metadata": {
        "id": "mR3LG6V0HDl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#STEMMING\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "# Create stemmer\n",
        "stemmer = PorterStemmer()\n",
        "# Sample text\n",
        "text = \"lle was studies and beautifully explained it easily.\"\n",
        "text1 = \"Artificial intelligence is studies  transforming the world  and display each word as a¬†separate¬†token\"\n",
        "tokens = word_tokenize(text1)\n",
        "# Apply stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "print(\"Original Words:\", tokens)\n",
        "print(\"After Stemming:\", stemmed_words)"
      ],
      "metadata": {
        "id": "P6bLkRT9A00U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c7dd309-f4ad-454f-8d2f-73db2c18f581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['Artificial', 'intelligence', 'is', 'studies', 'transforming', 'the', 'world', 'and', 'display', 'each', 'word', 'as', 'a', 'separate', 'token']\n",
            "After Stemming: ['artifici', 'intellig', 'is', 'studi', 'transform', 'the', 'world', 'and', 'display', 'each', 'word', 'as', 'a', 'separ', 'token']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LEMMATIZATION\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download(\"omw-1.4\")\n",
        "#Create lemmatizer\n",
        "lemmatizer= WordNetLemmatizer()\n",
        "#Sample text\n",
        "text = \"The studies were running better than expected boys gives given played.\"\n",
        "tokens =word_tokenize(text)\n",
        "#Apply lemmatization\n",
        "lem_words =[lemmatizer.lemmatize(word) for word in tokens]\n",
        "#lem_words1 =[lemmatizer.lemmatize(word) for word in lem_words]\n",
        "print(\"orginal words :\",tokens)\n",
        "print(\"After lemmatization:\", (lem_words))\n",
        "\n"
      ],
      "metadata": {
        "id": "mYHEbRUxFWii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb67d3d-e64c-4c4d-a647-00ff0a57b706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orginal words : ['The', 'studies', 'were', 'running', 'better', 'than', 'expected', 'boys', 'gives', 'given', 'played', '.']\n",
            "After lemmatization: ['The', 'study', 'were', 'running', 'better', 'than', 'expected', 'boy', 'give', 'given', 'played', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download(\"omw-1.4\")\n",
        "#Create lemmatizer\n",
        "lemmatizer= WordNetLemmatizer()\n",
        "#create Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "#Sample text\n",
        "text = input(\"Enter your Sentence ->\")\n",
        "tokens =word_tokenize(text)\n",
        "#Apply lemmatization\n",
        "lem_words =[lemmatizer.lemmatize(word) for word in tokens]\n",
        "#Apply Stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "print(\"orginal words :\",tokens)\n",
        "print(\"After Stemming:\", stemmed_words)\n",
        "print(\"After lemmatization:\", lem_words)"
      ],
      "metadata": {
        "id": "DZdcNpi6ISTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac38eaa-a040-473a-c615-bae3181be2ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Sentence ->dshbfxcb\n",
            "orginal words : ['dshbfxcb']\n",
            "After Stemming: ['dshbfxcb']\n",
            "After lemmatization: ['dshbfxcb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "#Sample text\n",
        "text = \"The students are studying NLP in the lab.\"\n",
        "text1 = \"Artificial Intelligence is transforming the world rapidly.\"\n",
        "#Tokenize\n",
        "tokens = word_tokenize(text)\n",
        "tokens1 = word_tokenize(text1)\n",
        "#Apply POS tagging\n",
        "pos_tags=nltk.pos_tag(tokens)\n",
        "pos_tags1 = nltk.pos_tag(tokens1)\n",
        "print(pos_tags)\n",
        "print(pos_tags1)"
      ],
      "metadata": {
        "id": "yW1vIaiNQn-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb48ff6-3f24-4a14-84fc-b2288a2ecab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('students', 'NNS'), ('are', 'VBP'), ('studying', 'VBG'), ('NLP', 'NNP'), ('in', 'IN'), ('the', 'DT'), ('lab', 'NN'), ('.', '.')]\n",
            "[('Artificial', 'JJ'), ('Intelligence', 'NNP'), ('is', 'VBZ'), ('transforming', 'VBG'), ('the', 'DT'), ('world', 'NN'), ('rapidly', 'RB'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Day2 : Understanding and Loading the Dataset\n",
        "import pandas as pd\n",
        "# Load dataset (after downloading from Kaggle)\n",
        "df = pd.read_csv('fake_job_postings.csv')\n",
        "#df = pd.read_csv('/job_descriptions.csv')\n",
        "# Display total number of records\n",
        "print(\"Total number of records:\", len(df))\n",
        "# Display basic info\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values per Column:\")\n",
        "print(df.isnull().sum())\n",
        "# Check distribution of target variable (assuming 'Work Type' or similar can indicate fake jobs for this dataset)\n",
        "# Since there is no 'fraudulent' column, I'll skip this part for now.\n",
        "# If you have a column that indicates real vs. fake jobs, please let me know its name.\n",
        "# print(\"\\nTarget (fraudulent) Distribution:\")\n",
        "# print(df['fraudulent'].value_counts()) # This line caused an error\n",
        "\n",
        "# Basic statistics\n",
        "print(\"\\nDataset Summary:\")\n",
        "print(df.describe(include='all'))\n",
        "\n",
        "# Display 3 examples of job descriptions (adjust column name if needed)\n",
        "print(\"\\nExamples of Job Descriptions:\")\n",
        "# Assuming 'Job Description' column exists based on df.columns output in cell YLWMaxC9DzGD\n",
        "if 'Job Description' in df.columns:\n",
        "    # Filter for potential \"fake\" job descriptions if a relevant column exists.\n",
        "    # Since there is no direct 'fraudulent' column, we'll just show general examples.\n",
        "    print(df['Job Description'].head(3).to_string())\n",
        "else:\n",
        "    print(\" 'Job Description' column not found.\")"
      ],
      "metadata": {
        "id": "q1-UInW766v4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fff7df3-a45f-4f55-97b0-2477d2a04903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of records: 17880\n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 17880 entries, 0 to 17879\n",
            "Data columns (total 18 columns):\n",
            " #   Column               Non-Null Count  Dtype \n",
            "---  ------               --------------  ----- \n",
            " 0   job_id               17880 non-null  int64 \n",
            " 1   title                17880 non-null  object\n",
            " 2   location             17534 non-null  object\n",
            " 3   department           6333 non-null   object\n",
            " 4   salary_range         2868 non-null   object\n",
            " 5   company_profile      14572 non-null  object\n",
            " 6   description          17879 non-null  object\n",
            " 7   requirements         15184 non-null  object\n",
            " 8   benefits             10668 non-null  object\n",
            " 9   telecommuting        17880 non-null  int64 \n",
            " 10  has_company_logo     17880 non-null  int64 \n",
            " 11  has_questions        17880 non-null  int64 \n",
            " 12  employment_type      14409 non-null  object\n",
            " 13  required_experience  10830 non-null  object\n",
            " 14  required_education   9775 non-null   object\n",
            " 15  industry             12977 non-null  object\n",
            " 16  function             11425 non-null  object\n",
            " 17  fraudulent           17880 non-null  int64 \n",
            "dtypes: int64(5), object(13)\n",
            "memory usage: 2.5+ MB\n",
            "None\n",
            "\n",
            "Missing Values per Column:\n",
            "job_id                     0\n",
            "title                      0\n",
            "location                 346\n",
            "department             11547\n",
            "salary_range           15012\n",
            "company_profile         3308\n",
            "description                1\n",
            "requirements            2696\n",
            "benefits                7212\n",
            "telecommuting              0\n",
            "has_company_logo           0\n",
            "has_questions              0\n",
            "employment_type         3471\n",
            "required_experience     7050\n",
            "required_education      8105\n",
            "industry                4903\n",
            "function                6455\n",
            "fraudulent                 0\n",
            "dtype: int64\n",
            "\n",
            "Dataset Summary:\n",
            "              job_id                    title         location department  \\\n",
            "count   17880.000000                    17880            17534       6333   \n",
            "unique           NaN                    11231             3105       1337   \n",
            "top              NaN  English Teacher Abroad   GB, LND, London      Sales   \n",
            "freq             NaN                      311              718        551   \n",
            "mean     8940.500000                      NaN              NaN        NaN   \n",
            "std      5161.655742                      NaN              NaN        NaN   \n",
            "min         1.000000                      NaN              NaN        NaN   \n",
            "25%      4470.750000                      NaN              NaN        NaN   \n",
            "50%      8940.500000                      NaN              NaN        NaN   \n",
            "75%     13410.250000                      NaN              NaN        NaN   \n",
            "max     17880.000000                      NaN              NaN        NaN   \n",
            "\n",
            "       salary_range                                    company_profile  \\\n",
            "count          2868                                              14572   \n",
            "unique          874                                               1709   \n",
            "top             0-0  We help teachers get safe &amp; secure jobs ab...   \n",
            "freq            142                                                726   \n",
            "mean            NaN                                                NaN   \n",
            "std             NaN                                                NaN   \n",
            "min             NaN                                                NaN   \n",
            "25%             NaN                                                NaN   \n",
            "50%             NaN                                                NaN   \n",
            "75%             NaN                                                NaN   \n",
            "max             NaN                                                NaN   \n",
            "\n",
            "                                              description  \\\n",
            "count                                               17879   \n",
            "unique                                              14801   \n",
            "top     Play with kids, get paid for it¬†Love travel? J...   \n",
            "freq                                                  379   \n",
            "mean                                                  NaN   \n",
            "std                                                   NaN   \n",
            "min                                                   NaN   \n",
            "25%                                                   NaN   \n",
            "50%                                                   NaN   \n",
            "75%                                                   NaN   \n",
            "max                                                   NaN   \n",
            "\n",
            "                                             requirements  \\\n",
            "count                                               15184   \n",
            "unique                                              11965   \n",
            "top     University degree required.¬†TEFL / TESOL / CEL...   \n",
            "freq                                                  410   \n",
            "mean                                                  NaN   \n",
            "std                                                   NaN   \n",
            "min                                                   NaN   \n",
            "25%                                                   NaN   \n",
            "50%                                                   NaN   \n",
            "75%                                                   NaN   \n",
            "max                                                   NaN   \n",
            "\n",
            "                   benefits  telecommuting  has_company_logo  has_questions  \\\n",
            "count                 10668   17880.000000      17880.000000   17880.000000   \n",
            "unique                 6203            NaN               NaN            NaN   \n",
            "top     See job description            NaN               NaN            NaN   \n",
            "freq                    726            NaN               NaN            NaN   \n",
            "mean                    NaN       0.042897          0.795302       0.491723   \n",
            "std                     NaN       0.202631          0.403492       0.499945   \n",
            "min                     NaN       0.000000          0.000000       0.000000   \n",
            "25%                     NaN       0.000000          1.000000       0.000000   \n",
            "50%                     NaN       0.000000          1.000000       0.000000   \n",
            "75%                     NaN       0.000000          1.000000       1.000000   \n",
            "max                     NaN       1.000000          1.000000       1.000000   \n",
            "\n",
            "       employment_type required_experience required_education  \\\n",
            "count            14409               10830               9775   \n",
            "unique               5                   7                 13   \n",
            "top          Full-time    Mid-Senior level  Bachelor's Degree   \n",
            "freq             11620                3809               5145   \n",
            "mean               NaN                 NaN                NaN   \n",
            "std                NaN                 NaN                NaN   \n",
            "min                NaN                 NaN                NaN   \n",
            "25%                NaN                 NaN                NaN   \n",
            "50%                NaN                 NaN                NaN   \n",
            "75%                NaN                 NaN                NaN   \n",
            "max                NaN                 NaN                NaN   \n",
            "\n",
            "                                   industry                function  \\\n",
            "count                                 12977                   11425   \n",
            "unique                                  131                      37   \n",
            "top     Information Technology and Services  Information Technology   \n",
            "freq                                   1734                    1749   \n",
            "mean                                    NaN                     NaN   \n",
            "std                                     NaN                     NaN   \n",
            "min                                     NaN                     NaN   \n",
            "25%                                     NaN                     NaN   \n",
            "50%                                     NaN                     NaN   \n",
            "75%                                     NaN                     NaN   \n",
            "max                                     NaN                     NaN   \n",
            "\n",
            "          fraudulent  \n",
            "count   17880.000000  \n",
            "unique           NaN  \n",
            "top              NaN  \n",
            "freq             NaN  \n",
            "mean        0.048434  \n",
            "std         0.214688  \n",
            "min         0.000000  \n",
            "25%         0.000000  \n",
            "50%         0.000000  \n",
            "75%         0.000000  \n",
            "max         1.000000  \n",
            "\n",
            "Examples of Job Descriptions:\n",
            " 'Job Description' column not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Dataset Exploration\n",
        "import pandas as pd\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"fake_job_postings.csv\")\n",
        "# Total number of records\n",
        "print(\"Total number of records:\", len(df))\n",
        "# Missing values per column\n",
        "print(\"\\nMissing Values per Column:\")\n",
        "print(df.isnull().sum())\n",
        "# Count of real vs fake jobs\n",
        "print(\"\\nTarget (fraudulent) Distribution:\")\n",
        "print(df['fraudulent'].value_counts())\n",
        "print(\"\\n0 = Real Jobs,  1 = Fake Jobs\")\n",
        "# Display 3 examples of fake job descriptions\n",
        "print(\"\\nExamples of Fake Job Descriptions:\")\n",
        "if 'description' in df.columns:\n",
        "    fake_jobs = df[df['fraudulent'] == 1]['description'].head(3)\n",
        "    print(fake_jobs.to_string(index=False))\n",
        "else:\n",
        "    print(\"'description' column not found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XglJLjeimqb",
        "outputId": "e48590a0-b547-4298-f795-3276e64f1e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of records: 17880\n",
            "\n",
            "Missing Values per Column:\n",
            "job_id                     0\n",
            "title                      0\n",
            "location                 346\n",
            "department             11547\n",
            "salary_range           15012\n",
            "company_profile         3308\n",
            "description                1\n",
            "requirements            2696\n",
            "benefits                7212\n",
            "telecommuting              0\n",
            "has_company_logo           0\n",
            "has_questions              0\n",
            "employment_type         3471\n",
            "required_experience     7050\n",
            "required_education      8105\n",
            "industry                4903\n",
            "function                6455\n",
            "fraudulent                 0\n",
            "dtype: int64\n",
            "\n",
            "Target (fraudulent) Distribution:\n",
            "fraudulent\n",
            "0    17014\n",
            "1      866\n",
            "Name: count, dtype: int64\n",
            "\n",
            "0 = Real Jobs,  1 = Fake Jobs\n",
            "\n",
            "Examples of Fake Job Descriptions:\n",
            "IC&amp;E Technician | Bakersfield, CA Mt. PosoP...\n",
            "The group has raised a fund for the purchase of...\n",
            "Technician Instrument &amp; ControlsLocation De...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Task 2\n",
        "Q1. Which feature has the most missing values?\n",
        "\n",
        "Answer:\n",
        "salary_range has the most missing values (around 15,000 rows).\n",
        "This happens because many job posts don‚Äôt disclose salary information.\n",
        "\n",
        "Q2. Why are job descriptions more useful than job titles for fake detection?\n",
        "\n",
        " Answer:\n",
        "Job descriptions contain detailed text that reveals the tone, intent, and wording used by the poster ‚Äî such as unrealistic promises (‚Äúwork from home and earn $1000/day‚Äù) or vague requirements ‚Äî which are strong clues of fraud.\n",
        "Job titles are short and often generic (e.g., ‚ÄúData Analyst,‚Äù ‚ÄúCustomer Support‚Äù), so they provide very limited information for detecting fake posts."
      ],
      "metadata": {
        "id": "eAZhUA6mjUi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Day 3: Text Cleaning and Preprocessing\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# Download resources (run once)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# Load dataset (same as Day 2)\n",
        "df = pd.read_csv('fake_job_postings.csv')\n",
        "# Define text cleaning function\n",
        "def clean_text(text):\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "    # 1. Lowercase\n",
        "    text = text.lower()\n",
        "    # 2. Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', ' ', text)\n",
        "    # 3. Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    # 4. Remove punctuation and numbers\n",
        "    text = re.sub(r'[%s\\d]' % re.escape(string.punctuation), ' ', text)\n",
        "    # 5. Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # 6. Remove stopwords and lemmatize\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
        "    return \" \".join(words)\n",
        "# Apply cleaning to key text columns\n",
        "df['clean_description'] = df['description'].apply(clean_text)\n",
        "# Show before and after\n",
        "print(\"Original Text:\\n\", df['description'].iloc[1][:300])\n",
        "print(\"\\nCleaned Text:\\n\", df['clean_description'].iloc[1][:300])\n",
        "# Check for any remaining issues\n",
        "print(\"\\nExample of Cleaned Data:\")\n",
        "print(df[['description', 'clean_description']].head(3))"
      ],
      "metadata": {
        "id": "CKLidUuFgPSF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94f2b576-05ac-4a3f-c45c-a9ba922567b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " Organised - Focused - Vibrant - Awesome!Do you have a passion for customer service? Slick typing skills? Maybe Account Management? ...And think administration is cooler than a polar bear on a jetski? Then we need to hear you!¬†We are the Cloud Video Production Service and opperating on a glodal level\n",
            "\n",
            "Cleaned Text:\n",
            " organised focused vibrant awesome passion customer service slick typing skill maybe account management think administration cooler polar bear jetski need hear cloud video production service opperating glodal level yeah pretty cool serious delivering world class product excellent customer service rap\n",
            "\n",
            "Example of Cleaned Data:\n",
            "                                         description  \\\n",
            "0  Food52, a fast-growing, James Beard Award-winn...   \n",
            "1  Organised - Focused - Vibrant - Awesome!Do you...   \n",
            "2  Our client, located in Houston, is actively se...   \n",
            "\n",
            "                                   clean_description  \n",
            "0  food fast growing james beard award winning on...  \n",
            "1  organised focused vibrant awesome passion cust...  \n",
            "2  client located houston actively seeking experi...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DAY 4\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Load dataset\n",
        "# -----------------------------\n",
        "# Make sure the CSV file is in your working directory\n",
        "df = pd.read_csv('fake_job_postings.csv')\n",
        "# Check what columns exist\n",
        "print(\"Available columns:\", df.columns.tolist())\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Create clean_description column\n",
        "# -----------------------------\n",
        "# Use 'description' column (update name if different in your dataset)\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()  # lowercase\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # keep only letters and spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # remove extra spaces\n",
        "    return text\n",
        "# Apply text cleaning\n",
        "df['clean_description'] = df['description'].apply(clean_text)\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Prepare text data\n",
        "# -----------------------------\n",
        "texts = df['clean_description'].fillna('').tolist()\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Bag-of-Words Representation\n",
        "# -----------------------------\n",
        "bow_vectorizer = CountVectorizer(max_features=2000)\n",
        "X_bow = bow_vectorizer.fit_transform(texts)\n",
        "print(\"‚úÖ BoW shape:\", X_bow.shape)\n",
        "print(\"üî§ Sample feature names (BoW):\", bow_vectorizer.get_feature_names_out()[:10])\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ TF-IDF Representation\n",
        "# -----------------------------\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=2000)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(texts)\n",
        "print(\"\\n‚úÖ TF-IDF shape:\", X_tfidf.shape)\n",
        "print(\"üî§ Sample feature names (TF-IDF):\", tfidf_vectorizer.get_feature_names_out()[:10])\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Compare first example\n",
        "# -----------------------------\n",
        "print(\"\\nüìò Example BoW vector (first row):\")\n",
        "print(X_bow[0].toarray())\n",
        "print(\"\\nüìó Example TF-IDF vector (first row):\")\n",
        "print(X_tfidf[0].toarray())"
      ],
      "metadata": {
        "id": "KqVCYvCBQHSW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1358e22c-8816-47bf-ab0b-43b825f47ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available columns: ['job_id', 'title', 'location', 'department', 'salary_range', 'company_profile', 'description', 'requirements', 'benefits', 'telecommuting', 'has_company_logo', 'has_questions', 'employment_type', 'required_experience', 'required_education', 'industry', 'function', 'fraudulent']\n",
            "‚úÖ BoW shape: (17880, 2000)\n",
            "üî§ Sample feature names (BoW): ['abilities' 'ability' 'able' 'about' 'above' 'academic' 'access'\n",
            " 'accordance' 'according' 'account']\n",
            "\n",
            "‚úÖ TF-IDF shape: (17880, 2000)\n",
            "üî§ Sample feature names (TF-IDF): ['abilities' 'ability' 'able' 'about' 'above' 'academic' 'access'\n",
            " 'accordance' 'according' 'account']\n",
            "\n",
            "üìò Example BoW vector (first row):\n",
            "[[0 0 0 ... 0 0 0]]\n",
            "\n",
            "üìó Example TF-IDF vector (first row):\n",
            "[[0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re, html\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "df = pd.read_csv(\"fake_job_postings.csv\")\n",
        "\n",
        "print(\"Total Records:\", len(df))\n",
        "print(\"\\nMissing Values per Column:\")\n",
        "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
        "\n",
        "print(\"\\nTarget Distribution (fraudulent column):\")\n",
        "print(df['fraudulent'].value_counts(normalize=True))\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean raw job posting text:\n",
        "    - Unescape HTML\n",
        "    - Lowercase\n",
        "    - Remove URLs, emails, numbers, punctuation\n",
        "    - Tokenize\n",
        "    - Remove stopwords\n",
        "    - Lemmatize using POS tagging\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = html.unescape(str(text))\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
        "\n",
        "    lemmatized = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return \" \".join(lemmatized)\n",
        "\n",
        "df[\"clean_description\"] = df[\"description\"].apply(clean_text)\n",
        "\n",
        "df[\"desc_word_count_before\"] = df[\"description\"].fillna(\"\").apply(lambda x: len(x.split()))\n",
        "df[\"desc_word_count_after\"] = df[\"clean_description\"].fillna(\"\").apply(lambda x: len(x.split()))\n",
        "\n",
        "print(\"\\nAverage word count before cleaning:\", df[\"desc_word_count_before\"].mean())\n",
        "print(\"Average word count after cleaning:\", df[\"desc_word_count_after\"].mean())\n",
        "\n",
        "sample = df[[\"description\", \"clean_description\"]].dropna().sample(3, random_state=42)\n",
        "print(\"\\nSample before vs after cleaning:\\n\", sample)\n"
      ],
      "metadata": {
        "id": "Aeo65y7NMfzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93023f50-e6e6-4358-c2d2-3e1f34b605b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Records: 17880\n",
            "\n",
            "Missing Values per Column:\n",
            "location                 346\n",
            "department             11547\n",
            "salary_range           15012\n",
            "company_profile         3308\n",
            "description                1\n",
            "requirements            2696\n",
            "benefits                7212\n",
            "employment_type         3471\n",
            "required_experience     7050\n",
            "required_education      8105\n",
            "industry                4903\n",
            "function                6455\n",
            "dtype: int64\n",
            "\n",
            "Target Distribution (fraudulent column):\n",
            "fraudulent\n",
            "0    0.951566\n",
            "1    0.048434\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Average word count before cleaning: 170.4459731543624\n",
            "Average word count after cleaning: 104.31068232662193\n",
            "\n",
            "Sample before vs after cleaning:\n",
            "                                              description  \\\n",
            "4708   Stylect is a dynamic startup that helps helps ...   \n",
            "11079  General Summary: Achieves maximum sales profit...   \n",
            "12357  At ustwo‚Ñ¢ you get to be yourself, whilst deliv...   \n",
            "\n",
            "                                       clean_description  \n",
            "4708   stylect dynamic startup help help woman discov...  \n",
            "11079  general summary achieves maximum sale profitab...  \n",
            "12357  ustwo get whilst delivering best work planet b...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Day 5: Logistic Regression Model for Fake Job Detection\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "# Load dataset (preprocessed with clean_description)\n",
        "#df = pd.read_csv('fake_job_postings.csv')\n",
        "df = df.dropna(subset=['clean_description'])\n",
        "# Feature extraction using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(df['clean_description'])\n",
        "y = df['fraudulent']\n",
        "# Split data into train & test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "#  Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#  Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# 6Ô∏è‚É£ Check example predictions\n",
        "test_samples = [\n",
        "    \"Work from home! Limited vacancies. Apply now.\",\n",
        "    \"We are hiring a data scientist for our Bangalore office.\"\n",
        "]\n",
        "sample_features = vectorizer.transform(test_samples)\n",
        "print(\"\\nSample Predictions:\", model.predict(sample_features))"
      ],
      "metadata": {
        "id": "Lr401gJoKi2h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9684d138-391f-48e5-e1e8-722da1038927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.9642058165548099\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98      3403\n",
            "           1       0.98      0.27      0.42       173\n",
            "\n",
            "    accuracy                           0.96      3576\n",
            "   macro avg       0.97      0.63      0.70      3576\n",
            "weighted avg       0.96      0.96      0.95      3576\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[3402    1]\n",
            " [ 127   46]]\n",
            "\n",
            "Sample Predictions: [0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Day 6: Decision Tree & Random Forest Models\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load preprocessed dataset\n",
        "#df = pd.read_csv('fake_job_postings.csv')\n",
        "#df = df.dropna(subset=['clean_description'])\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=3000)\n",
        "X = vectorizer.fit_transform(df['clean_description'])\n",
        "y = df['fraudulent']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 1Ô∏è‚É£ Decision Tree\n",
        "dt = DecisionTreeClassifier(max_depth=20, random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "# 2Ô∏è‚É£ Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "\n",
        "print(\"\\nRandom Forest Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# Confusion matrix visualization\n",
        "cm = confusion_matrix(y_test, y_pred_rf)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Random Forest Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n",
        "\n",
        "# Feature Importance\n",
        "importances = rf.feature_importances_\n",
        "indices = importances.argsort()[-10:][::-1]\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.barh(range(len(indices)), importances[indices], align='center')\n",
        "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "plt.title(\"Top 10 Important Words (Random Forest)\")\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FwpOtk9L6bvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "<meta charset=\"UTF-8\">\n",
        "<title>Prediction Result</title>\n",
        "<style>\n",
        "        body { font-family: Arial; background-color: #f9f9f9; text-align: center; padding: 50px; }\n",
        "        .card { background: white; padding: 30px; border-radius: 12px; width: 50%; margin: auto; box-shadow: 0px 4px 6px rgba(0,0,0,0.1); }\n",
        "        h2 { color: #333; }\n",
        "        .fake { color: red; font-weight: bold; }\n",
        "        .real { color: green; font-weight: bold; }\n",
        "        a { display: inline-block; margin-top: 20px; text-decoration: none; color: #007bff; }\n",
        "        a:hover { text-decoration: underline; }\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"card\">\n",
        "<h2>Prediction Result</h2>\n",
        "<p><b>Job Description:</b> {{ description }}</p>\n",
        "<h3>Prediction:\n",
        "            {% if label == 'Fake Job' %}\n",
        "<span class=\"fake\">{{ label }}</span>\n",
        "            {% else %}\n",
        "<span class=\"real\">{{ label }}</span>\n",
        "            {% endif %}\n",
        "</h3>\n",
        "<p>Confidence: {{ confidence }}%</p>\n",
        "<a href=\"/\">üîô Go Back</a>\n",
        "</div>\n",
        "</body>\n",
        "</html>"
      ],
      "metadata": {
        "id": "Qe1LUqxv6GPl",
        "outputId": "2547a118-eb35-457e-b709-f766cd3aa667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid decimal literal (ipython-input-429432304.py, line 8)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-429432304.py\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    .card { background: white; padding: 30px; border-radius: 12px; width: 50%; margin: auto; box-shadow: 0px 4px 6px rgba(0,0,0,0.1); }\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('templates/result.html', 'w') as f:\n",
        "    f.write(\"\"\"<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "<meta charset=\"UTF-8\">\n",
        "<title>Prediction Result</title>\n",
        "<style>\n",
        "    body { font-family: Arial; background-color: #f9f9f9; text-align: center; padding: 50px; }\n",
        "    .card { background: white; padding: 30px; border-radius: 12px; width: 50%; margin: auto; box-shadow: 0px 4px 6px rgba(0,0,0,0.1); }\n",
        "    h2 { color: #333; }\n",
        "    .fake { color: red; font-weight: bold; }\n",
        "    .real { color: green; font-weight: bold; }\n",
        "    a { display: inline-block; margin-top: 20px; text-decoration: none; color: #007bff; }\n",
        "    a:hover { text-decoration: underline; }\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"card\">\n",
        "<h2>Prediction Result</h2>\n",
        "<p><b>Job Description:</b> {{ description }}</p>\n",
        "<h3>Prediction:\n",
        "    {% if label == 'Fake Job' %}\n",
        "        <span class=\"fake\">{{ label }}</span>\n",
        "    {% else %}\n",
        "        <span class=\"real\">{{ label }}</span>\n",
        "    {% endif %}\n",
        "</h3>\n",
        "<p>Confidence: {{ confidence }}%</p>\n",
        "<a href=\"/\">üîô Go Back</a>\n",
        "</div>\n",
        "</body>\n",
        "</html>\"\"\")\n"
      ],
      "metadata": {
        "id": "4oCrWv992cO_",
        "outputId": "455cfc82-5ee0-45ba-9fd1-971fbf009a66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'templates/result.html'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1863940575.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'templates/result.html'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     f.write(\"\"\"<!DOCTYPE html>\n\u001b[1;32m      3\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mhtml\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mmeta\u001b[0m \u001b[0mcharset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"UTF-8\"\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'templates/result.html'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, render_template, request, redirect, url_for\n",
        "\n",
        "import joblib\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load model and vectorizer\n",
        "\n",
        "model = joblib.load('fake_job_model.pkl')\n",
        "\n",
        "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
        "\n",
        "# Global counters (basic dashboard)\n",
        "\n",
        "fake_count = 0\n",
        "\n",
        "real_count = 0\n",
        "\n",
        "@app.route('/')\n",
        "\n",
        "def home():\n",
        "\n",
        "    return render_template('index.html', fake=fake_count, real=real_count)\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "\n",
        "def predict():\n",
        "\n",
        "    global fake_count, real_count\n",
        "\n",
        "    job_desc = request.form.get('job_description', '').strip()\n",
        "\n",
        "    # Error handling\n",
        "\n",
        "    if not job_desc or len(job_desc.split()) < 5:\n",
        "\n",
        "        return render_template('index.html', error=\"Please enter a detailed job description (at least 5 words).\",\n",
        "\n",
        "                               fake=fake_count, real=real_count)\n",
        "\n",
        "    # Transform & predict\n",
        "\n",
        "    X_input = vectorizer.transform([job_desc])\n",
        "\n",
        "    pred = model.predict(X_input)[0]\n",
        "\n",
        "    prob = model.predict_proba(X_input)[0][1]\n",
        "\n",
        "    label = \"Fake Job\" if pred == 1 else \"Real Job\"\n",
        "\n",
        "    confidence = round(prob * 100, 2) if pred == 1 else round((1 - prob) * 100, 2)\n",
        "\n",
        "    # Update counters\n",
        "\n",
        "    if pred == 1:\n",
        "\n",
        "        fake_count += 1\n",
        "\n",
        "    else:\n",
        "\n",
        "        real_count += 1\n",
        "\n",
        "    return render_template('result.html', label=label, confidence=confidence,\n",
        "\n",
        "                           description=job_desc, fake=fake_count, real=real_count)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    app.run(debug=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "EowPf5ZcGaXf",
        "outputId": "29755eb3-61a7-4a54-ccb5-8fe23a11099d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'fake_job_model.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2303113932.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load model and vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fake_job_model.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf_vectorizer.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode, ensure_native_byte_order)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_native_byte_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_native_byte_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             with _validate_fileobject_and_memmap(f, filename, mmap_mode) as (\n\u001b[1;32m    737\u001b[0m                 \u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fake_job_model.pkl'"
          ]
        }
      ]
    }
  ]
}